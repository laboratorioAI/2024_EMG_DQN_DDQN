%

%% Set-Up

addpath("Helpers\", "Data\", "Reset\", "Step\", "Reward\")

obsInfo = rlNumericSpec([13 24 8], Name='emgFeatures');
actInfo = rlFiniteSetSpec([1 2 3 4 5 6], Name='gestures');

obsInfos = {obsInfo, obsInfo};
actInfos = {actInfo, actInfo};

env = rlMultiAgentFunctionEnv(obsInfos, actInfos, @stepFnc, @resetFnc);
cnn = targetCNN();

NNOptions = rlRepresentationOptions(...
    LearnRate=10e-4,...
    GradientThreshold=1, ...
    Optimizer="adam",...
    GradientThresholdMethod="l2norm",...
    UseDevice="gpu");

NeuralN = rlQValueRepresentation( ...
    cnn, ...
    obsInfo, ...
    actInfo,...
    'Observation','state', ...
    NNOptions);

%% Params

targetSmoothFactor = 1e-4;
miniBatchSize = 32;
numStepsLookAhead = 1;
discountFactor = 0.98;
experienceBufferLength = 150;
epsilonDecay = 1e-5;

%% DQN

agentOptionsDQN = rlDQNAgentOptions( ...
    UseDoubleDQN=false, ...
    TargetSmoothFactor=targetSmoothFactor, ...
    MiniBatchSize=miniBatchSize, ...
    NumStepsToLookAhead=numStepsLookAhead, ...
    DiscountFactor=discountFactor, ...
    SaveExperienceBufferWithAgent=true, ...
    ExperienceBufferLength=experienceBufferLength);

agentOptionsDQN.EpsilonGreedyExploration.EpsilonDecay = epsilonDecay;

agentDQN = rlDQNAgent(NeuralN,agentOptionsDQN);
getAction(agentDQN,rand(obsInfo.Dimension));

%% DDQN

agentOptionsDDQN = rlDQNAgentOptions( ...
    UseDoubleDQN=true, ...
    TargetSmoothFactor=targetSmoothFactor, ...
    MiniBatchSize=miniBatchSize, ...
    NumStepsToLookAhead=numStepsLookAhead, ...
    DiscountFactor=discountFactor, ...
    SaveExperienceBufferWithAgent=true, ...
    ExperienceBufferLength=experienceBufferLength);

agentOptionsDDQN.EpsilonGreedyExploration.EpsilonDecay = epsilonDecay;

agentDDQN = rlDQNAgent(NeuralN,agentOptionsDDQN);
getAction(agentDDQN,rand(obsInfo.Dimension));

%% Training

trainOpts = rlMultiAgentTrainingOptions(...
    AgentGroups={[1 2]},...
    LearningStrategy=("decentralized"),... centralized: Both DQN or DQN not mixed.
    MaxEpisodes=150*306*2,... 150*306 MaxStepsPerEpisode=150 Removed due to use isDone flag to iterate over all sample window
    ScoreAveragingWindowLength=5,...
    StopTrainingCriteria="AverageReward",...
    StopTrainingValue=25,...
    SaveAgentCriteria="EpisodeReward",...
    SaveAgentValue=20,...
    Verbose=true,...
    SaveAgentDirectory=pwd + "\Snapshots",...
    Plots="training-progress");

%{
evl = rlEvaluator( ...
    EvaluationStatisticType="MaxEpisodeReward",...
    NumEpisodes=5, ... How many episodes will be evaluated
    EvaluationFrequency=5, ...
    MaxStepsPerEpisode=25);
%}

clear resetFnc;
clear stepFnc;

results = train([agentDQN, agentDDQN], env, trainOpts);

%% Save agents

save("DQN-" + string(datetime("today")) + ".mat", "agentDQN");
save("DDQN-" + string(datetime("today")) + ".mat", "agentDDQN");
save("NET-" + string(datetime("today")) + ".mat", "cnn")
