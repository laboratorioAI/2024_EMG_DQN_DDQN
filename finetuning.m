%% Set-Up
close all;
clear all;
clc;

addpath("Helpers\", "Data\", "Reset\", "Step\", "Reward\")

obsInfo = rlNumericSpec([13 24 8], Name='emgFeatures');
actInfo = rlFiniteSetSpec([1 2 3 4 5 6], Name='gestures');

obsInfos = {obsInfo, obsInfo};
actInfos = {actInfo, actInfo};

env = rlMultiAgentFunctionEnv(obsInfos, actInfos, @stepFnc, @resetFnc);

%% Load pre-trained model

dagnet = layerGraph(coder.loadDeepLearningNetwork('model_s30_e10.mat'));

newInputLayer = imageInputLayer([13 24 8], 'Normalization', 'none', 'Name', 'state');
dagnet = replaceLayer(dagnet, 'data', newInputLayer);

%% Network options

NNOptions = rlRepresentationOptions(...
    LearnRate=1e-6,... 7?
    GradientThreshold=1, ...
    Optimizer="adam",...
    GradientThresholdMethod="l2norm",...
    UseDevice="gpu");

NeuralN = rlQValueRepresentation( ...
    dagnet, ...
    obsInfo, ...
    actInfo,...
    'Observation','state', ...
    NNOptions);

%% Params

targetSmoothFactor = 1e-4;
miniBatchSize = 32;
numStepsLookAhead = 1;
discountFactor = 0.98;
experienceBufferLength = 50;
epsilonDecay = 1e-5;

%% DQN

agentOptionsDQN = rlDQNAgentOptions( ...
    UseDoubleDQN=false, ...
    TargetSmoothFactor=targetSmoothFactor, ...
    MiniBatchSize=miniBatchSize, ...
    NumStepsToLookAhead=numStepsLookAhead, ...
    DiscountFactor=discountFactor, ...
    SaveExperienceBufferWithAgent=true, ...
    ExperienceBufferLength=experienceBufferLength);

agentOptionsDQN.EpsilonGreedyExploration.EpsilonDecay = epsilonDecay;
agentDQN = rlDQNAgent(NeuralN,agentOptionsDQN);

%% DDQN

agentOptionsDDQN = rlDQNAgentOptions( ...
    UseDoubleDQN=true, ...
    TargetSmoothFactor=targetSmoothFactor, ...
    MiniBatchSize=miniBatchSize, ...
    NumStepsToLookAhead=numStepsLookAhead, ...
    DiscountFactor=discountFactor, ...
    SaveExperienceBufferWithAgent=true, ...
    ExperienceBufferLength=experienceBufferLength);

agentOptionsDDQN.EpsilonGreedyExploration.EpsilonDecay = epsilonDecay;
agentDDQN = rlDQNAgent(NeuralN,agentOptionsDDQN);

%% Training
maxEpisodes = prod([150 1 10]); % maxSamples, maxUsers, maxIterations

trainOpts = rlMultiAgentTrainingOptions(...
    AgentGroups={[1 2]},...
    LearningStrategy=("decentralized"),...
    MaxEpisodes=maxEpisodes,...
    ScoreAveragingWindowLength=150,...
    StopTrainingCriteria="EpisodeCount",...
    StopTrainingValue=maxEpisodes,...
    SaveAgentCriteria="EpisodeReward",...
    SaveAgentValue=1,...
    Verbose=true,...
    SaveAgentDirectory=pwd + "\Snapshots",...
    Plots="training-progress");

%% Train

clear resetFnc;
clear stepFnc;

results = train([agentDQN, agentDDQN], env, trainOpts);

save("results.mat", "results")
save("agentDQN.mat", "agentDQN");
save("agentDDQN.mat", "agentDDQN");
